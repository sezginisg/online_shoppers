{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mod 5 Online Shoppers Intent Project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents:\n",
    "1. \n",
    "2.\n",
    "3.\n",
    "4.\n",
    "5.\n",
    "6.\n",
    "7.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Business Case: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from textwrap import wrap\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "from IPython.display import Image  \n",
    "from pydotplus import graph_from_dot_data\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12330, 18)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('online_shoppers_intention.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Scrubbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Administrative             0\n",
       "Administrative_Duration    0\n",
       "Informational              0\n",
       "Informational_Duration     0\n",
       "ProductRelated             0\n",
       "ProductRelated_Duration    0\n",
       "BounceRates                0\n",
       "ExitRates                  0\n",
       "PageValues                 0\n",
       "SpecialDay                 0\n",
       "Month                      0\n",
       "OperatingSystems           0\n",
       "Browser                    0\n",
       "Region                     0\n",
       "TrafficType                0\n",
       "VisitorType                0\n",
       "Weekend                    0\n",
       "Revenue                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for Duplicated values\n",
    "df.duplicated().sum()\n",
    "# 125 values duplicated\n",
    "# Will leave them in as they could be duplicates by chance and \n",
    "# they are very small portion of our data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Data Set Information:\n",
    "\n",
    "The dataset consists of feature vectors belonging to 12,330 sessions.\n",
    "The dataset was formed so that each session\n",
    "would belong to a different user in a 1-year period to avoid\n",
    "any tendency to a specific campaign, special day, user\n",
    "profile, or period.\n",
    "\n",
    "\n",
    "### 4.2 Attribute Information:\n",
    "\n",
    "The dataset consists of 10 numerical and 8 categorical attributes.\n",
    "The 'Revenue' attribute can be used as the class label.\n",
    "\n",
    "\"Administrative\", \"Administrative Duration\", \"Informational\", \"Informational Duration\", \"Product Related\" and \"Product Related Duration\" represent the number of different types of pages visited by the visitor in that session and total time spent in each of these page categories. The values of these features are derived from the URL information of the pages visited by the user and updated in real time when a user takes an action, e.g. moving from one page to another. The \"Bounce Rate\", \"Exit Rate\" and \"Page Value\" features represent the metrics measured by \"Google Analytics\" for each page in the e-commerce site. The value of \"Bounce Rate\" feature for a web page refers to the percentage of visitors who enter the site from that page and then leave (\"bounce\") without triggering any other requests to the analytics server during that session. The value of \"Exit Rate\" feature for a specific web page is calculated as for all pageviews to the page, the percentage that were the last in the session. The \"Page Value\" feature represents the average value for a web page that a user visited before completing an e-commerce transaction. The \"Special Day\" feature indicates the closeness of the site visiting time to a specific special day (e.g. Mother’s Day, Valentine's Day) in which the sessions are more likely to be finalized with transaction. The value of this attribute is determined by considering the dynamics of e-commerce such as the duration between the order date and delivery date. For example, for Valentina’s day, this value takes a nonzero value between February 2 and February 12, zero before and after this date unless it is close to another special day, and its maximum value of 1 on February 8. The dataset also includes operating system, browser, region, traffic type, visitor type as returning or new visitor, a Boolean value indicating whether the date of the visit is weekend, and month of the year.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12330 entries, 0 to 12329\n",
      "Data columns (total 18 columns):\n",
      "Administrative             12330 non-null int64\n",
      "Administrative_Duration    12330 non-null float64\n",
      "Informational              12330 non-null int64\n",
      "Informational_Duration     12330 non-null float64\n",
      "ProductRelated             12330 non-null int64\n",
      "ProductRelated_Duration    12330 non-null float64\n",
      "BounceRates                12330 non-null float64\n",
      "ExitRates                  12330 non-null float64\n",
      "PageValues                 12330 non-null float64\n",
      "SpecialDay                 12330 non-null float64\n",
      "Month                      12330 non-null object\n",
      "OperatingSystems           12330 non-null int64\n",
      "Browser                    12330 non-null int64\n",
      "Region                     12330 non-null int64\n",
      "TrafficType                12330 non-null int64\n",
      "VisitorType                12330 non-null object\n",
      "Weekend                    12330 non-null bool\n",
      "Revenue                    12330 non-null bool\n",
      "dtypes: bool(2), float64(7), int64(7), object(2)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Screen for Categorical variables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Administrative</th>\n",
       "      <th>Administrative_Duration</th>\n",
       "      <th>Informational</th>\n",
       "      <th>Informational_Duration</th>\n",
       "      <th>ProductRelated</th>\n",
       "      <th>ProductRelated_Duration</th>\n",
       "      <th>BounceRates</th>\n",
       "      <th>ExitRates</th>\n",
       "      <th>PageValues</th>\n",
       "      <th>SpecialDay</th>\n",
       "      <th>Month</th>\n",
       "      <th>OperatingSystems</th>\n",
       "      <th>Browser</th>\n",
       "      <th>Region</th>\n",
       "      <th>TrafficType</th>\n",
       "      <th>VisitorType</th>\n",
       "      <th>Weekend</th>\n",
       "      <th>Revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>627.500000</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Returning_Visitor</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Administrative  Administrative_Duration  Informational  \\\n",
       "0               0                      0.0              0   \n",
       "1               0                      0.0              0   \n",
       "2               0                      0.0              0   \n",
       "3               0                      0.0              0   \n",
       "4               0                      0.0              0   \n",
       "\n",
       "   Informational_Duration  ProductRelated  ProductRelated_Duration  \\\n",
       "0                     0.0               1                 0.000000   \n",
       "1                     0.0               2                64.000000   \n",
       "2                     0.0               1                 0.000000   \n",
       "3                     0.0               2                 2.666667   \n",
       "4                     0.0              10               627.500000   \n",
       "\n",
       "   BounceRates  ExitRates  PageValues  SpecialDay Month  OperatingSystems  \\\n",
       "0         0.20       0.20         0.0         0.0   Feb                 1   \n",
       "1         0.00       0.10         0.0         0.0   Feb                 2   \n",
       "2         0.20       0.20         0.0         0.0   Feb                 4   \n",
       "3         0.05       0.14         0.0         0.0   Feb                 3   \n",
       "4         0.02       0.05         0.0         0.0   Feb                 3   \n",
       "\n",
       "   Browser  Region  TrafficType        VisitorType  Weekend  Revenue  \n",
       "0        1       1            1  Returning_Visitor    False    False  \n",
       "1        2       1            2  Returning_Visitor    False    False  \n",
       "2        1       9            3  Returning_Visitor    False    False  \n",
       "3        2       2            4  Returning_Visitor    False    False  \n",
       "4        3       1            4  Returning_Visitor     True    False  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Administrative             0.002190\n",
       "Administrative_Duration    0.270479\n",
       "Informational              0.001379\n",
       "Informational_Duration     0.102028\n",
       "ProductRelated             0.025223\n",
       "ProductRelated_Duration    0.774615\n",
       "BounceRates                0.151825\n",
       "ExitRates                  0.387429\n",
       "PageValues                 0.219303\n",
       "SpecialDay                 0.000487\n",
       "Month                      0.000811\n",
       "OperatingSystems           0.000649\n",
       "Browser                    0.001054\n",
       "Region                     0.000730\n",
       "TrafficType                0.001622\n",
       "VisitorType                0.000243\n",
       "Weekend                    0.000162\n",
       "Revenue                    0.000162\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we are checking the ratio of unique values to the total number count for each column\n",
    "df.nunique()/df.count()\n",
    "# proportions of nuniques to total counts < 0.05 suggest categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop to visually inspect value counts for all variables\n",
    "# for col in df.columns:\n",
    "#     print(f'This is {col} value counts: \\n{df[col].value_counts()}.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset categorical and continuous features from dataframe for visualisations:\n",
    "df_cont = df[['Administrative_Duration', 'Informational_Duration', 'ProductRelated_Duration', 'ExitRates', 'PageValues', 'BounceRates']]\n",
    "df_cat = df.drop(df_cont.columns, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature and Target subsetting:\n",
    "X = df.drop(columns=['Revenue'], axis =1)\n",
    "y = df['Revenue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_test_split and create a global seed\n",
    "seed=42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9864, 401)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot encode the categorical variables in the training data and show the resulting DataFrame with proper column names\n",
    "ohe = OneHotEncoder()\n",
    "#subset continuous and categorical variables:\n",
    "train_cont = X_train[['Administrative_Duration', 'Informational_Duration', 'ProductRelated_Duration', 'ExitRates', 'PageValues', 'BounceRates']]\n",
    "train_cat = X_train.drop(df_cont.columns, axis=1)\n",
    "\n",
    "\n",
    "#Fit transform the variables and place them in a dataframe\n",
    "encoded_vars = ohe.fit_transform(train_cat).toarray()\n",
    "ohe_df = pd.DataFrame(encoded_vars, columns=ohe.get_feature_names(train_cat.columns))\n",
    "ohe_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9864, 407)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reset index and make a copy of continuous dataframe.\n",
    "X_train_ohe = train_cont.copy()\n",
    "X_train_ohe.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Concat into continuous and encoded categoricals into one training dataset:\n",
    "X_train_ohe = pd.concat([X_train_ohe, ohe_df], axis=1)\n",
    "X_train_ohe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Decision Tree fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5674, 0.8692, 0.7445)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decision tree classifier fit \n",
    "clf = DecisionTreeClassifier(random_state = seed, criterion='entropy')\n",
    "\n",
    "clf.fit(X_train_ohe, y_train)\n",
    "#KFold cross validator instantiated:\n",
    "cv = KFold(n_splits=5, random_state=seed)\n",
    "\n",
    "# Use Crossvalidation to obtain Performance metrics: F1 Score and Accuracy\n",
    "F1_score = round(np.mean(cross_val_score(clf, X_train_ohe, y_train, cv=cv, scoring='f1')), 4)\n",
    "Acc_score = round(np.mean(cross_val_score(clf, X_train_ohe, y_train, cv=cv, scoring='accuracy')), 4)\n",
    "roc_AUC_score = round(np.mean(cross_val_score(clf, X_train_ohe, y_train, cv=cv, scoring='roc_auc')), 4)\n",
    "#Print F1 and Accuracy and ROC_AUC score for crossvalidation baseline decision tree\n",
    "F1_score, Acc_score, roc_AUC_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5.4 Baseline Model - Plot the Decision Tree\n",
    "\n",
    "# Create DOT Data\n",
    "# dot_data = export_graphviz(clf, out_file=None,\n",
    "#                            feature_names=X_train_ohe.columns,\n",
    "#                            class_names=np.unique(y).astype('str'),\n",
    "#                            filled=True, rounded=True, special_characters=True)\n",
    "\n",
    "# Draw Graph \n",
    "# graph = graph_from_dot_data(dot_data) \n",
    "\n",
    "# Show graph\n",
    "# Image(graph.create_png())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0 Exploring Potential Improvements on Baseline Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Decision tree with Gini impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5622, 0.8688, 0.7403)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gini impurity Decision tree classifier fit \n",
    "clf1 = DecisionTreeClassifier(random_state = seed, criterion='gini')\n",
    "clf1.fit(X_train_ohe, y_train)\n",
    "\n",
    "# Use Crossvalidation to obtain Performance metrics: F1 Score and Accuracy\n",
    "F1_score = round(np.mean(cross_val_score(clf1, X_train_ohe, y_train, cv=cv, scoring='f1')), 4)\n",
    "Acc_score = round(np.mean(cross_val_score(clf1, X_train_ohe, y_train, cv=cv, scoring='accuracy')), 4)\n",
    "roc_AUC_score = round(np.mean(cross_val_score(clf1, X_train_ohe, y_train, cv=cv, scoring='roc_auc')), 4)\n",
    "\n",
    "#Print F1 and Accuracy and ROC_AUC score for crossvalidation baseline decision tree\n",
    "F1_score, Acc_score, roc_AUC_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Grid Search CV for Decision tree with Entropy impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=DecisionTreeClassifier(class_weight=None,\n",
       "                                              criterion='entropy',\n",
       "                                              max_depth=None, max_features=None,\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              presort=False, random_state=42,\n",
       "                                              splitter='best'),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'criterion': ['entropy'],\n",
       "                         'max_depth': [None, 2, 3, 4, 5, 6],\n",
       "                         'min_samples_leaf': [1, 2, 3, 4, 5, 6],\n",
       "                         'min_samples_split': [2, 5, 10]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entropy impurity performed better. Let's implement hyperparameter tuning with combinatoric grid searching.\n",
    "\n",
    "#initial Param grid:\n",
    "\n",
    "dt_param_grid = {\n",
    "    'criterion': ['entropy'],\n",
    "    'max_depth': [None, 2, 3, 4, 5, 6],\n",
    "    'min_samples_split': [10, 500, 1000]\n",
    "}\n",
    "\n",
    "\n",
    "#insantiate the GridSearchCV\n",
    "dt_grid_search = GridSearchCV(clf, dt_param_grid, cv=3, return_train_score=True)\n",
    "\n",
    "# Fit to the data\n",
    "dt_grid_search.fit(X_train_ohe, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Score: 91.45%\n",
      "Best Parameter Combination Found During Grid Search:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy',\n",
       " 'max_depth': 3,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean training score\n",
    "dt_gs_training_score = np.mean(dt_grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "\n",
    "print(f\"Mean Training Score: {dt_gs_training_score :.2%}\")\n",
    "\n",
    "# Print best parameter combination found during grid search:\n",
    "print(\"Best Parameter Combination Found During Grid Search:\")\n",
    "dt_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Score:0.6239144956579826, Precision Score:0.6949404761904762, Accuracy Score0.9013584752635847, F1 Score 0.6575149595212954\n"
     ]
    }
   ],
   "source": [
    "# Primary Param grid prediction scores: (Accuracy, Precision, Recall and F1)\n",
    "predictions = dt_grid_search.best_estimator_.predict(X_train_ohe)\n",
    "accuracy_score1 = accuracy_score(y_train, predictions)\n",
    "recall_score1 = recall_score(y_train, predictions)\n",
    "precision_score1 = precision_score(y_train, predictions)\n",
    "F1_score1 = f1_score(y_train, predictions)\n",
    "print(f'Recall Score:{recall_score1}, Precision Score:{precision_score1}, Accuracy Score{accuracy_score1}, F1 Score {F1_score1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=DecisionTreeClassifier(class_weight=None,\n",
       "                                              criterion='entropy',\n",
       "                                              max_depth=None, max_features=None,\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              presort=False, random_state=42,\n",
       "                                              splitter='best'),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'criterion': ['entropy'], 'max_depth': [3],\n",
       "                         'min_samples_split': [2, 3, 5, 7, 9]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Secondary Param_Grid:\n",
    "\n",
    "dt_param_grid1 = {\n",
    "    'criterion': ['entropy'],\n",
    "    'max_depth': [3],\n",
    "    'min_samples_split': [2,3,5,7,9]\n",
    "}\n",
    "dt_grid_search1 = GridSearchCV(clf, dt_param_grid1, cv=3, return_train_score=True)\n",
    "\n",
    "# Fit to the data\n",
    "dt_grid_search1.fit(X_train_ohe, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training Score: 90.04%\n",
      "Best Parameter Combination Found During Grid Search:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy', 'max_depth': 3, 'min_samples_split': 2}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Secondary Param Grid Mean train score \n",
    "dt_gs_training_score1 = np.mean(dt_grid_search1.cv_results_['mean_train_score'])\n",
    "\n",
    "print(f\"Mean Training Score: {dt_gs_training_score1 :.2%}\")\n",
    "\n",
    "# Secondary Param grid best combinations:\n",
    "print(\"Best Parameter Combination Found During Grid Search:\")\n",
    "dt_grid_search1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Score:0.6239144956579826, Precision Score:0.6949404761904762, Accuracy Score0.9013584752635847, F1 Score 0.6575149595212954\n"
     ]
    }
   ],
   "source": [
    "#Predictions for secondary parameter grid and its related Recall, precision accuracy and F1 scores:\n",
    "predictions = dt_grid_search1.best_estimator_.predict(X_train_ohe)\n",
    "accuracy_score2 = accuracy_score(y_train, predictions)\n",
    "recall_score2 = recall_score(y_train, predictions)\n",
    "precision_score2 = precision_score(y_train, predictions)\n",
    "F1_score2 = f1_score(y_train, predictions)\n",
    "print(f'Recall Score:{recall_score2}, Precision Score:{precision_score2}, Accuracy Score{accuracy_score2}, F1 Score {F1_score2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Roc plot:\n",
    "# scores(model, X_train_ohe,y_train)\n",
    "# roc_plot(model,X_train_ohe,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3 Ensemble Methods - Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cross Validation Score for Random Forest Classifier:  88.39%\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Random Forest and cross validate fit with the training data:\n",
    "rf_clf = RandomForestClassifier()\n",
    "mean_rf_cv_score = np.mean(cross_val_score(rf_clf, X_train_ohe, y_train, cv=3))\n",
    "\n",
    "#print resulting mean score.\n",
    "print(f\"Mean Cross Validation Score for Random Forest Classifier: {mean_rf_cv_score : .2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Param Grid 1:\n",
    "\n",
    "# Create the random grid\n",
    "Random_grid = {'bootstrap': [True, False],\n",
    "             'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "             'max_features': ['auto', 'sqrt'],\n",
    "             'min_samples_leaf': [1, 2, 4],\n",
    "             'min_samples_split': [2, 5, 10],\n",
    "             'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n",
    "              }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
